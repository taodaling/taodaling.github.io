---
categories: ML
layout: post
---

- Table
{:toc}

# 概念

统计学习是指：从给定的、有限的、用于学习的训练数据集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为假设空间；应用某个评价准则，从假设空间中选取一个最优模型；最优模型的选取由算法实现。

## 分类

按学习分类
- 监督学习
- 无监督学习
- 强化学习
- 半监督学习
- 主动学习

按模型分类
- 概率模型和非概率模型
- 线性模型和非线性模型
- 参数化模型和非参数化模型

按算法分类
- 在线学习和批量学习

按技巧分类
- 贝叶斯学习
- 核方法

## 三要素

### 模型

决策函数组成的假设空间

$$
F=\left\{f_\theta|Y=f_\theta(X),\theta\in R^n\right\}
$$

条件概率组成的假设空间

$$
F=\left\{P_\theta|P_\theta(Y|X),\theta\in R^n\right\}
$$

其中$\theta$是模型参数，它的取值范围$R^n$称为参数空间。

### 策略

损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。

损失函数记作$L(Y,f(X))$。常用的有
- 0-1损失函数：$L(Y,f(X))=[Y=f(X)]$
- 平方损失函数：$L(Y,f(X))=(Y-f(X))^2$
- 绝对损失函数：$L(Y,f(X))=\|Y-f(X)\|$
- 对数损失函数：$L(Y,P(Y\|X))=-\log P(Y\|X)$

损失函数越小，模型就越好。模型的输入$(X,Y)$遵循联合分布$P(X,Y)$，所以损失函数的期望为：

$$
\begin{aligned}
&R_{\exp}(f)
\\=&E_P[L(Y,f(X))]
\\=&\int_{X\times Y}L(y,f(x))P(x,y)dxdy
\end{aligned}
$$

学习的目标是寻找期望风险最小的模型。但是由于联合分布是未知的，因此我们无法比较不同模型的期望风险。

对于训练数据集

$$
T=\left\{(x_i,y_i)|i\in [1,N]\right\}
$$

模型$f(X)$关于$T$的平均损失称为经验风险

$$
R_{\mathrm{emp}}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))
$$

根据大数定理，当样本数量$N$趋于无穷时，经验风险趋于期望风险。我们可以用经验风险来估计期望风险。

监督学习的基本策略有两种：
- 经验风险最小化（ERM）
- 结构风险最小化（SRM）

经验风险最小化选择经验风险最小的模型，它在样本容量足够大的情况下效果很好，但是样本容量较小的时候会发生过拟合。

结构风险最小化可以避免过拟合，结构风险是在经验风险的基础上加上表示模型复杂度的正则化项。

### 算法

我们需要设计良好的算法，在假设空间中找到风险最小的模型。

## 过拟合

### 正则化

随着模型的复杂度上升，训练误差会逐步降低，但是测试误差在选择模型复杂度低于真实模型复杂度的时候降低，高于真实模型复杂度的时候升高。

一种避免过拟合的方法是正则化，采用结构风险最小化策略。其一般形式为：

$$
\min\limits_{f\in F}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)
$$

正则化项可以采用参数向量的$L_1$或$L_2$范数。

从贝叶斯估计的角度来看，正则化项对应于模型的先验概率，假设越复杂的模型出现的概率越低。

### 交叉验证

另外一种方式是交叉验证。

如果给定的样本数量足够，可以将数据集随机地切分为三部分：训练集，验证集，测试集。其中训练集用于训练模型，验证集用于挑选模型，而测试集用于对模型的泛化能力进行评估。

但是实际上数据是不充足的，为了选择更好的模型，可以采用交叉验证的方式。

- 简单交叉验证：随机地将已知数据分为两部分，一部分作为训练集，另一部分作为测试集。然后用训练集在各种条件下训练模型，之后选择在测试集上测试误差最小的模型
- S折交叉验证：随机将数据切分为$S$个互不相交、大小相同的子集；然后利用$S-1$个子集的数据训练模型，利用剩下的一个子集测试模型；总共可以进行$S$次，选择$S$次评测中平均测试误差最小的模型。
- 留一交叉验证：S则交叉验证的特殊情况，此时$S=N$（即每个子集大小都是$1$），适用于数据极度缺乏的情况。

## 泛化能力

泛化误差用来评价学习方法的泛化能力。泛化误差等价于模型的期望风险，我们实践中一般用测试误差来估计泛化误差。

学习方法的泛化能力的比较一般通过研究泛化误差的概率上界进行，简称为泛化误差上界。

对于二类分类问题，如果假设空间是有限的且大小为$d$，对任意一个函数$f\in F$，至少以概率$1-\delta$使得下面不等式成立，其中$0<\delta<1$：

$$
R(f)\leq \widehat{R}(f)+\sqrt{\frac{1}{2N}\log \frac{d}{\delta}} 
$$

# 感知机

感知机是二类分类的线性分类模型。输出值为$\left\\{+1,-1\right\\}$。

感知机学习旨在求出将训练数据进行线性划分的分离超平面。

假设输入空间是$X\subseteq R^n$，输出空间是$Y=\left\\{+1,-1\right\\}$。输出$x\in X$表示实例的特征向量，对应于输入空间的点，输出$y\in Y$表示实例的类别。由输入空间到输出空间的如下函数：

$$
f(x)=\mathrm{sign}(w\cdot x+b)
$$

称为感知机。其中$w$和$b$称为感知机模型参数，$w\in R^n$叫做权值向量，$b\in R$叫做偏置。

感知机的几何解释是线性方程$w\cdot x+b$是$R^n$中的一个超平面$S$，其中$w$是超平面的法向量，$b$是超平面的截距。这个超平面将特征空间划分为两个部分，两个部分中的点分别被分类为正负两类。而超平面$S$称为分离超平面。

## 损失函数

感知机的损失函数的定义，一个自然的选择是误分类点的总数，但是这样的损失函数不是参数$w$和$b$的连续可导函数，不易优化。另外一个选择是使用误分类点到超平面$S$的总距离。

定义任一点$x_0$到超平面$S$的距离为：

$$
\frac{1}{\|w\|}|w\cdot x_0+b|
$$

其次对误分类的数据$(x_i,y_i)$来说有：

$$
-y_i(w\cdot x_i+b)>0
$$

因此误分类点到超平面$S$的距离是

$$
-\frac{1}{\|w\|}y_i(w\cdot x_i+b)
$$

记所有误分类点的集合为$M$，那么总距离为

$$
-\frac{1}{\|w\|}\sum_{x_i\in M}y_i(w\cdot x_i+b)
$$

定义损失函数为

$$
L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)
$$

## 学习算法

感知机学习算法是误分类驱动的，具体采用随机梯度下降法。首先任意选择一个超平面$w_0,b_0$，之后不断用梯度下降法极小化损失函数。极小化过程中不是一次使$M$中所有误分类点的梯度下降，而是一次随机选取一个误分类点并使其梯度下降。

假设误分类点集合$M$是固定的，那么损失函数$L(w,b)$的梯度由

$$
\nabla_wL(w,b)=-\sum_{x_i\in M}y_ix_i\\
\nabla_bL(w,b)=-\sum_{x_i\in M}y_i
$$

随机选取一个误分类点$(x_i,y_i)$，对$w,b$进行更新：

$$
w\leftarrow w+\eta y_ix_i\\
b\leftarrow b+\eta y_i
$$

其中$\eta$是步长（学习率），取值范围为$0<\eta\leq 1$。

重复上述流程直到不存在误分类点。

## 算法的收敛性

设训练数据集$T$是线性可分的，则：

(1) 存在满足条件$\|\widehat{w}_{opt}\|=1$的超平面$\widehat{w}_{opt}\cdot \widehat{x}=0$将训练数据完全正确分开，且存在$\gamma>0$，对于任意$1\leq i\leq N$，有

$$
y_i(\widehat{w}_{opt}\cdot \widehat{x}_i)\geq \gamma
$$

(2) 令$R=\max\limits_{1\leq i\leq N}\\|\widehat{x}_i\\|$，则感知机算法在训练数据集上的误分类次数$k$满足不等式：

$$
k\leq (\frac{R}{\gamma})^2
$$
